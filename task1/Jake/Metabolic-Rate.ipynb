{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mygene\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple, Union, Any\n",
    "import json\n",
    "from sparse_lmm import VariableSelection\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from typing import Callable, Optional, List, Tuple, Union, Any\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def get_relevant_filepaths(cohort_dir):\n",
    "    \"\"\"Find the file paths of a SOFT file and a matrix file from the given data directory of a cohort.\n",
    "    If there are multiple SOFT files or matrix files, simply choose the first one. May be replaced by better\n",
    "    strategies later.\n",
    "    \"\"\"\n",
    "    files = os.listdir(cohort_dir)\n",
    "    soft_files = [f for f in files if 'soft' in f.lower()]\n",
    "    matrix_files = [f for f in files if 'matrix' in f.lower()]\n",
    "    assert len(soft_files) > 0 and len(matrix_files) > 0\n",
    "    soft_file_path = os.path.join(cohort_dir, soft_files[0])\n",
    "    matrix_file_path = os.path.join(cohort_dir, matrix_files[0])\n",
    "\n",
    "    return soft_file_path, matrix_file_path\n",
    "\n",
    "def line_generator(source, source_type):\n",
    "    \"\"\"Generator that yields lines from a file or a string.\n",
    "\n",
    "    Parameters:\n",
    "    - source: File path or string content.\n",
    "    - source_type: 'file' or 'string'.\n",
    "    \"\"\"\n",
    "    if source_type == 'file':\n",
    "        with gzip.open(source, 'rt') as f:\n",
    "            for line in f:\n",
    "                yield line.strip()\n",
    "    elif source_type == 'string':\n",
    "        for line in source.split('\\n'):\n",
    "            yield line.strip()\n",
    "    else:\n",
    "        raise ValueError(\"source_type must be 'file' or 'string'\")\n",
    "\n",
    "def filter_content_by_prefix(\n",
    "    source: str,\n",
    "    prefixes_a: List[str],\n",
    "    prefixes_b: Optional[List[str]] = None,\n",
    "    unselect: bool = False,\n",
    "    source_type: str = 'file',\n",
    "    return_df_a: bool = True,\n",
    "    return_df_b: bool = True\n",
    ") -> Tuple[Union[str, pd.DataFrame], Optional[Union[str, pd.DataFrame]]]:\n",
    "    \"\"\"\n",
    "    Filters rows from a file or a list of strings based on specified prefixes.\n",
    "\n",
    "    Parameters:\n",
    "    - source (str): File path or string content to filter.\n",
    "    - prefixes_a (List[str]): Primary list of prefixes to filter by.\n",
    "    - prefixes_b (Optional[List[str]]): Optional secondary list of prefixes to filter by.\n",
    "    - unselect (bool): If True, selects rows that do not start with the specified prefixes.\n",
    "    - source_type (str): 'file' if source is a file path, 'string' if source is a string of text.\n",
    "    - return_df_a (bool): If True, returns filtered content for prefixes_a as a pandas DataFrame.\n",
    "    - return_df_b (bool): If True, and if prefixes_b is provided, returns filtered content for prefixes_b as a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: A tuple where the first element is the filtered content for prefixes_a, and the second element is the filtered content for prefixes_b.\n",
    "    \"\"\"\n",
    "    filtered_lines_a = []\n",
    "    filtered_lines_b = []\n",
    "    prefix_set_a = set(prefixes_a)\n",
    "    if prefixes_b is not None:\n",
    "        prefix_set_b = set(prefixes_b)\n",
    "\n",
    "    # Use generator to get lines\n",
    "    for line in line_generator(source, source_type):\n",
    "        matched_a = any(line.startswith(prefix) for prefix in prefix_set_a)\n",
    "        if matched_a != unselect:\n",
    "            filtered_lines_a.append(line)\n",
    "        if prefixes_b is not None:\n",
    "            matched_b = any(line.startswith(prefix) for prefix in prefix_set_b)\n",
    "            if matched_b != unselect:\n",
    "                filtered_lines_b.append(line)\n",
    "\n",
    "    filtered_content_a = '\\n'.join(filtered_lines_a)\n",
    "    if return_df_a:\n",
    "        filtered_content_a = pd.read_csv(io.StringIO(filtered_content_a), delimiter='\\t', low_memory=False, on_bad_lines='skip')\n",
    "    filtered_content_b = None\n",
    "    if filtered_lines_b:\n",
    "        filtered_content_b = '\\n'.join(filtered_lines_b)\n",
    "        if return_df_b:\n",
    "            filtered_content_b = pd.read_csv(io.StringIO(filtered_content_b), delimiter='\\t', low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "    return filtered_content_a, filtered_content_b\n",
    "\n",
    "def get_background_and_clinical_data(file_path,\n",
    "                                     prefixes_a=['!Series_title', '!Series_summary', '!Series_overall_design'],\n",
    "                                     prefixes_b=['!Sample_geo_accession', '!Sample_characteristics_ch1']):\n",
    "    \"\"\"Extract from a matrix file the background information about the dataset, and sample characteristics data\"\"\"\n",
    "    background_info, clinical_data = filter_content_by_prefix(file_path, prefixes_a, prefixes_b, unselect=False,\n",
    "                                                              source_type='file',\n",
    "                                                              return_df_a=False, return_df_b=True)\n",
    "    return background_info, clinical_data\n",
    "\n",
    "def get_unique_values_by_row(dataframe, max_len=30):\n",
    "    \"\"\"\n",
    "    Organize the unique values in each row of the given dataframe, to get a dictionary\n",
    "    :param dataframe:\n",
    "    :param max_len:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if '!Sample_geo_accession' in dataframe.columns:\n",
    "        dataframe = dataframe.drop(columns=['!Sample_geo_accession'])\n",
    "    unique_values_dict = {}\n",
    "    for index, row in dataframe.iterrows():\n",
    "        unique_values = list(row.unique())[:max_len]\n",
    "        unique_values_dict[index] = unique_values\n",
    "    return unique_values_dict\n",
    "\n",
    "def check_rows_and_columns(dataframe, display=False):\n",
    "    \"\"\"\n",
    "    Get the lists of row names and column names of a dataset, and optionally observe them.\n",
    "    :param dataframe:\n",
    "    :param display:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataframe_rows = dataframe.index.tolist()\n",
    "    if display:\n",
    "        print(f\"The dataset has {len(dataframe_rows)} rows, such as {dataframe_rows[:20]}\")\n",
    "    dataframe_cols = dataframe.columns.tolist()\n",
    "    if display:\n",
    "        print(f\"\\nThe dataset has {len(dataframe_cols)} columns, such as {dataframe_cols[:20]}\")\n",
    "    return dataframe_rows, dataframe_cols\n",
    "\n",
    "def preview_df(df, n=5):\n",
    "    return df.head(n).to_dict(orient='list')\n",
    "\n",
    "def xena_convert_trait(row_index: str):\n",
    "    \"\"\"\n",
    "    Convert the trait information from Sample IDs to labels depending on the last two digits.\n",
    "    Tumor types range from 01 - 09, normal types from 10 - 19.\n",
    "    :param row_index: the index value of a row\n",
    "    :return: the converted value\n",
    "    \"\"\"\n",
    "    last_two_digits = int(row_index[-2:])\n",
    "\n",
    "    if 1 <= last_two_digits <= 9:\n",
    "        return 1\n",
    "    elif 10 <= last_two_digits <= 19:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def xena_convert_gender(cell: str):\n",
    "    \"\"\"Convert the cell content about gender to a binary value\n",
    "    \"\"\"\n",
    "    if isinstance(cell, str):\n",
    "        cell = cell.lower()\n",
    "\n",
    "    if cell == \"female\":\n",
    "        return 0\n",
    "    elif cell == \"male\":\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def xena_convert_age(cell: str):\n",
    "    \"\"\"Convert the cell content about age to a numerical value using regular expression\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\d+', str(cell))\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def xena_select_clinical_features(clinical_df, trait, age_col=None, gender_col=None):\n",
    "    feature_list = []\n",
    "    trait_data = clinical_df.index.to_series().apply(xena_convert_trait).rename(trait)\n",
    "    feature_list.append(trait_data)\n",
    "    if age_col:\n",
    "        age_data = clinical_df[age_col].apply(xena_convert_age).rename(\"Age\")\n",
    "        feature_list.append(age_data)\n",
    "    if gender_col:\n",
    "        gender_data = clinical_df[gender_col].apply(xena_convert_gender).rename(\"Gender\")\n",
    "        feature_list.append(gender_data)\n",
    "    selected_clinical_df = pd.concat(feature_list, axis=1)\n",
    "    return selected_clinical_df\n",
    "\n",
    "def normalize_gene_symbols(gene_symbols, batch_size=1000):\n",
    "    \"\"\"Normalize human gene symbols in batches using the 'mygenes' library\"\"\"\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    normalized_genes = {}\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(gene_symbols), batch_size):\n",
    "        batch = gene_symbols[i:i + batch_size]\n",
    "        results = mg.querymany(batch, scopes='symbol', fields='symbol', species='human')\n",
    "\n",
    "        # Update the normalized_genes dictionary with results from this batch\n",
    "        for gene in results:\n",
    "            normalized_genes[gene['query']] = gene.get('symbol', None)\n",
    "\n",
    "    # Return the normalized symbols in the same order as the input\n",
    "    return [normalized_genes.get(symbol) for symbol in gene_symbols]\n",
    "\n",
    "def normalize_gene_symbols_in_index(gene_df):\n",
    "    \"\"\"Normalize the human gene symbols at the index of a dataframe, and replace the index with its normalized version.\n",
    "    Remove the rows where the index failed to be normalized.\"\"\"\n",
    "    normalized_gene_list = normalize_gene_symbols(gene_df.index.tolist())\n",
    "    assert len(normalized_gene_list) == len(gene_df.index)\n",
    "    gene_df.index = normalized_gene_list\n",
    "    gene_df = gene_df[gene_df.index.notnull()]\n",
    "    return gene_df\n",
    "\n",
    "def judge_binary_variable_biased(dataframe, col_name, min_proportion=0.1, min_num=5):\n",
    "    \"\"\"\n",
    "    Check if the distribution of a binary variable in the dataset is too biased to be usable for analysis\n",
    "    :param dataframe:\n",
    "    :param col_name:\n",
    "    :param min_proportion:\n",
    "    :param min_num:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    label_counter = dataframe[col_name].value_counts()\n",
    "    total_samples = len(dataframe)\n",
    "    rare_label_num = label_counter.min()\n",
    "    rare_label = label_counter.idxmin()\n",
    "    rare_label_proportion = rare_label_num / total_samples\n",
    "\n",
    "    print(\n",
    "        f\"For the feature \\'{col_name}\\', the least common label is '{rare_label}' with {rare_label_num} occurrences. This represents {rare_label_proportion:.2%} of the dataset.\")\n",
    "\n",
    "    biased = (len(label_counter) < 2) or ((rare_label_proportion < min_proportion) and (rare_label_num < min_num))\n",
    "    return bool(biased)\n",
    "\n",
    "def judge_continuous_variable_biased(dataframe, col_name):\n",
    "    \"\"\"Check if the distribution of a continuous variable in the dataset is too biased to be usable for analysis.\n",
    "    As a starting point, we consider it biased if all values are the same. For the next step, maybe ask GPT to judge\n",
    "    based on quartile statistics combined with its common sense knowledge about this feature.\n",
    "    \"\"\"\n",
    "    quartiles = dataframe[col_name].quantile([0.25, 0.5, 0.75])\n",
    "    min_value = dataframe[col_name].min()\n",
    "    max_value = dataframe[col_name].max()\n",
    "\n",
    "    # Printing quartile information\n",
    "    print(f\"Quartiles for '{col_name}':\")\n",
    "    print(f\"  25%: {quartiles[0.25]}\")\n",
    "    print(f\"  50% (Median): {quartiles[0.5]}\")\n",
    "    print(f\"  75%: {quartiles[0.75]}\")\n",
    "    print(f\"Min: {min_value}\")\n",
    "    print(f\"Max: {max_value}\")\n",
    "\n",
    "    biased = min_value == max_value\n",
    "\n",
    "    return bool(biased)\n",
    "\n",
    "def judge_and_remove_biased_features(df, trait, trait_type):\n",
    "    assert trait_type in [\"binary\", \"continuous\"], f\"The trait must be either a binary or a continuous variable!\"\n",
    "    if trait_type == \"binary\":\n",
    "        trait_biased = judge_binary_variable_biased(df, trait)\n",
    "    else:\n",
    "        trait_biased = judge_continuous_variable_biased(df, trait)\n",
    "    if trait_biased:\n",
    "        print(f\"The distribution of the feature \\'{trait}\\' in this dataset is severely biased.\\n\")\n",
    "    else:\n",
    "        print(f\"The distribution of the feature \\'{trait}\\' in this dataset is fine.\\n\")\n",
    "    if \"Age\" in df.columns:\n",
    "        age_biased = judge_continuous_variable_biased(df, 'Age')\n",
    "        if age_biased:\n",
    "            print(f\"The distribution of the feature \\'Age\\' in this dataset is severely biased.\\n\")\n",
    "            df = df.drop(columns='Age')\n",
    "        else:\n",
    "            print(f\"The distribution of the feature \\'Age\\' in this dataset is fine.\\n\")\n",
    "    if \"Gender\" in df.columns:\n",
    "        gender_biased = judge_binary_variable_biased(df, 'Gender')\n",
    "        if gender_biased:\n",
    "            print(f\"The distribution of the feature \\'Gender\\' in this dataset is severely biased.\\n\")\n",
    "            df = df.drop(columns='Gender')\n",
    "        else:\n",
    "            print(f\"The distribution of the feature \\'Gender\\' in this dataset is fine.\\n\")\n",
    "\n",
    "    return trait_biased, df\n",
    "\n",
    "def save_cohort_info(cohort: str, info_path: str, is_available: bool, is_biased: Optional[bool] = None,\n",
    "                     df: Optional[pd.DataFrame] = None, note: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Add or update information about the usability and quality of a dataset for statistical analysis.\n",
    "\n",
    "    Parameters:\n",
    "    cohort (str): A unique identifier for the dataset.\n",
    "    info_path (str): File path to the JSON file where records are stored.\n",
    "    is_available (bool): Indicates whether both the genetic data and trait data are available in the dataset, and can be\n",
    "     preprocessed into a dataframe.\n",
    "    is_biased (bool, optional): Indicates whether the dataset is too biased to be usable.\n",
    "        Required if `is_available` is True.\n",
    "    df (pandas.DataFrame, optional): The preprocessed dataset. Required if `is_available` is True.\n",
    "    note (str, optional): Additional notes about the dataset.\n",
    "\n",
    "    Returns:\n",
    "    None: The function does not return a value but updates or creates a record in the specified JSON file.\n",
    "    \"\"\"\n",
    "    if is_available:\n",
    "        assert (df is not None) and (is_biased is not None), \"'df' and 'is_biased' should be provided if this cohort \" \\\n",
    "                                                             \"is relevant.\"\n",
    "    is_usable = is_available and (not is_biased)\n",
    "    new_record = {\"is_usable\": is_usable,\n",
    "                  \"is_available\": is_available,\n",
    "                  \"is_biased\": is_biased if is_available else None,\n",
    "                  \"has_age\": \"Age\" in df.columns if is_available else None,\n",
    "                  \"has_gender\": \"Gender\" in df.columns if is_available else None,\n",
    "                  \"sample_size\": len(df) if is_available else None,\n",
    "                  \"note\": note}\n",
    "\n",
    "    if not os.path.exists(info_path):\n",
    "        with open(info_path, 'w') as file:\n",
    "            json.dump({}, file)\n",
    "        print(f\"A new JSON file was created at: {info_path}\")\n",
    "\n",
    "    with open(info_path, \"r\") as file:\n",
    "        records = json.load(file)\n",
    "    records[cohort] = new_record\n",
    "\n",
    "    temp_path = info_path + \".tmp\"\n",
    "    try:\n",
    "        with open(temp_path, 'w') as file:\n",
    "            json.dump(records, file)\n",
    "        os.replace(temp_path, info_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        raise\n",
    "\n",
    "def get_feature_data(clinical_df, row_id, feature, convert_fn):\n",
    "    \"\"\"select the row corresponding to a feature in the sample characteristics dataframe, and convert the feature into\n",
    "    a binary or continuous variable\"\"\"\n",
    "    clinical_df = clinical_df.iloc[row_id:row_id + 1].drop(columns=['!Sample_geo_accession'], errors='ignore')\n",
    "    clinical_df.index = [feature]\n",
    "    clinical_df = clinical_df.applymap(convert_fn)\n",
    "\n",
    "    return clinical_df\n",
    "\n",
    "def geo_select_clinical_features(clinical_df: pd.DataFrame, trait: str, trait_row: int,\n",
    "                                 convert_trait: Callable,\n",
    "                                 age_row: Optional[int] = None,\n",
    "                                 convert_age: Optional[Callable] = None,\n",
    "                                 gender_row: Optional[int] = None,\n",
    "                                 convert_gender: Optional[Callable] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts and processes specific clinical features from a DataFrame representing\n",
    "    sample characteristics in the GEO database series.\n",
    "\n",
    "    Parameters:\n",
    "    - clinical_df (pd.DataFrame): DataFrame containing clinical data.\n",
    "    - trait (str): The trait of interest.\n",
    "    - trait_row (int): Row identifier for the trait in the DataFrame.\n",
    "    - convert_trait (Callable): Function to convert trait data into a desired format.\n",
    "    - age_row (int, optional): Row identifier for age data. Default is None.\n",
    "    - convert_age (Callable, optional): Function to convert age data. Default is None.\n",
    "    - gender_row (int, optional): Row identifier for gender data. Default is None.\n",
    "    - convert_gender (Callable, optional): Function to convert gender data. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the selected and processed clinical features.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "\n",
    "    trait_data = get_feature_data(clinical_df, trait_row, trait, convert_trait)\n",
    "    feature_list.append(trait_data)\n",
    "    if age_row is not None:\n",
    "        age_data = get_feature_data(clinical_df, age_row, 'Age', convert_age)\n",
    "        feature_list.append(age_data)\n",
    "    if gender_row is not None:\n",
    "        gender_data = get_feature_data(clinical_df, gender_row, 'Gender', convert_gender)\n",
    "        feature_list.append(gender_data)\n",
    "\n",
    "    selected_clinical_df = pd.concat(feature_list, axis=0)\n",
    "    return selected_clinical_df\n",
    "\n",
    "def get_genetic_data(file_path):\n",
    "    \"\"\"Read the gene expression data into a dataframe, and adjust its format\"\"\"\n",
    "    genetic_data = pd.read_csv(file_path, compression='gzip', skiprows=52, comment='!', delimiter='\\t')\n",
    "    genetic_data = genetic_data.dropna()\n",
    "    genetic_data = genetic_data.rename(columns={'ID_REF': 'ID'}).astype({'ID': 'str'})\n",
    "    genetic_data.set_index('ID', inplace=True)\n",
    "\n",
    "    return genetic_data\n",
    "\n",
    "def get_gene_annotation(file_path, prefixes=['^', '!', '#']):\n",
    "    \"\"\"Extract from a SOFT file the gene annotation data\"\"\"\n",
    "    gene_metadata = filter_content_by_prefix(file_path, prefixes_a=prefixes, unselect=True, source_type='file',\n",
    "                                             return_df_a=True)\n",
    "    return gene_metadata[0]\n",
    "\n",
    "def get_gene_mapping(annotation, prob_col, gene_col):\n",
    "    \"\"\"Process the gene annotation to get the mapping between gene names and gene probes.\n",
    "    \"\"\"\n",
    "    mapping_data = annotation.loc[:, [prob_col, gene_col]]\n",
    "    mapping_data = mapping_data.dropna()\n",
    "    mapping_data = mapping_data.rename(columns={gene_col: 'Gene'}).astype({'ID': 'str'})\n",
    "\n",
    "    return mapping_data\n",
    "\n",
    "def apply_gene_mapping(expression_df, mapping_df):\n",
    "    \"\"\"\n",
    "    Converts measured data about gene probes into gene expression data.\n",
    "    Handles the potential many-to-many relationship between probes and genes.\n",
    "\n",
    "    Parameters:\n",
    "    expression_df (DataFrame): A DataFrame with gene expression data, indexed by 'ID'.\n",
    "    mapping_df (DataFrame): A DataFrame mapping 'ID' to 'Gene', with 'ID' as a column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with mean gene expression values, indexed by 'Gene'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a regex pattern for splitting gene names\n",
    "    split_pattern = r';|\\|+|/{2,}|,|\\[|\\]|\\(|\\)'\n",
    "\n",
    "    # Split the 'Gene' column in 'mapping_df' using the regex pattern\n",
    "    mapping_df['Gene'] = mapping_df['Gene'].str.split(split_pattern)\n",
    "    mapping_df = mapping_df.explode('Gene')\n",
    "\n",
    "    # Set 'ID' as the index of 'mapping_df' for merging\n",
    "    mapping_df.set_index('ID', inplace=True)\n",
    "\n",
    "    # Merge 'mapping_df' with 'expression_df' by their indices\n",
    "    merged_df = mapping_df.join(expression_df)\n",
    "\n",
    "    # Group by 'Gene' and calculate the mean expression values\n",
    "    gene_expression_df = merged_df.groupby('Gene').mean().dropna()\n",
    "\n",
    "    return gene_expression_df\n",
    "\n",
    "def geo_merge_clinical_genetic_data(clinical_df, genetic_df):\n",
    "    \"\"\"\n",
    "    Merge the clinical features and gene expression features from two dataframes into one dataframe\n",
    "    \"\"\"\n",
    "    if 'ID' in genetic_df.columns:\n",
    "        genetic_df = genetic_df.rename(columns={'ID': 'Gene'})\n",
    "    if 'Gene' in genetic_df.columns:\n",
    "        genetic_df = genetic_df.set_index('Gene')\n",
    "    merged_data = pd.concat([clinical_df, genetic_df], axis=0).T.dropna()\n",
    "    return merged_data\n",
    "\n",
    "def read_json_to_dataframe(json_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a JSON file and converts it into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    json_file (str): The path to the JSON file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame with the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return pd.DataFrame.from_dict(data, orient='index').reset_index().rename(columns={'index': 'cohort_id'})\n",
    "\n",
    "def filter_and_rank_cohorts(json_file: str, condition: Union[str, None] = None) -> Tuple[\n",
    "    Union[str, None], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads a JSON file, filters cohorts based on usability and an optional condition, then ranks them by sample size.\n",
    "\n",
    "    Args:\n",
    "    json_file (str): The path to the JSON file containing the data.\n",
    "    condition (str, optional): An additional condition for filtering. If None, only 'is_usable' is considered.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: A tuple containing the best cohort ID (str or None if no suitable cohort is found) and\n",
    "           the filtered and ranked DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the JSON file into a DataFrame\n",
    "    df = read_json_to_dataframe(json_file)\n",
    "\n",
    "    if condition:\n",
    "        filtered_df = df[(df['is_usable'] == True) & (df[condition] == True)]\n",
    "    else:\n",
    "        filtered_df = df[df['is_usable'] == True]\n",
    "\n",
    "    ranked_df = filtered_df.sort_values(by='sample_size', ascending=False)\n",
    "    best_cohort_id = ranked_df.iloc[0]['cohort_id'] if not ranked_df.empty else None\n",
    "\n",
    "    return best_cohort_id, ranked_df\n",
    "\n",
    "def detect_batch_effect(X):\n",
    "    \"\"\"\n",
    "    Detect potential batch effects in a dataset using eigenvalues of XX^T.\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): A feature matrix with shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "    bool: True if a potential batch effect is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Computing XX^T\n",
    "    XXt = np.dot(X, X.T)\n",
    "\n",
    "    # Compute the eigenvalues of XX^T\n",
    "    eigen_values = np.linalg.eigvalsh(XXt)  # Using eigvalsh since XX^T is symmetric\n",
    "    eigen_values = sorted(eigen_values, reverse=True)\n",
    "\n",
    "    # Check for large gaps in the eigenvalues\n",
    "    for i in range(len(eigen_values) - 1):\n",
    "        gap = eigen_values[i] - eigen_values[i + 1]\n",
    "        if gap > 1 / n_samples:  # You may need to adjust this threshold\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def normalize_data(X_train, X_test=None):\n",
    "    \"\"\"Compute the mean and standard deviation statistics of the training data, use them to normalize the training data,\n",
    "    and optionally the test data\"\"\"\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "\n",
    "    # Handling columns with std = 0\n",
    "    std_no_zero = np.where(std == 0, 1, std)\n",
    "\n",
    "    # Normalize X_train\n",
    "    X_train_normalized = (X_train - mean) / std_no_zero\n",
    "    # Set normalized values to 0 where std was 0\n",
    "    X_train_normalized[:, std == 0] = 0\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test_normalized = (X_test - mean) / std_no_zero\n",
    "        X_test_normalized[:, std == 0] = 0\n",
    "    else:\n",
    "        X_test_normalized = None\n",
    "\n",
    "    return X_train_normalized, X_test_normalized\n",
    "\n",
    "\n",
    "class ResidualizationRegressor:\n",
    "    def __init__(self, regression_model_constructor, params=None):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.regression_model = regression_model_constructor(**params)\n",
    "        self.beta_Z = None  # Coefficients for regression of Y on Z\n",
    "        self.beta_X = None  # Coefficients for regression of residual on X\n",
    "        self.neg_log_p_values = None  # Negative logarithm of p-values\n",
    "        self.p_values = None  # Actual p-values\n",
    "\n",
    "    def _reshape_data(self, data):\n",
    "        \"\"\"\n",
    "        Reshape the data to ensure it's in the correct format (2D array).\n",
    "\n",
    "        :param data: The input data (can be 1D or 2D array).\n",
    "        :return: Reshaped 2D array.\n",
    "        \"\"\"\n",
    "        if data.ndim == 1:\n",
    "            return data.reshape(-1, 1)\n",
    "        return data\n",
    "\n",
    "    def _reshape_output(self, data):\n",
    "        \"\"\"\n",
    "        Reshape the output data to ensure it's in the correct format (1D array).\n",
    "\n",
    "        :param data: The output data (can be 1D or 2D array).\n",
    "        :return: Reshaped 1D array.\n",
    "        \"\"\"\n",
    "        if data.ndim == 2 and data.shape[1] == 1:\n",
    "            return data.ravel()\n",
    "        return data\n",
    "\n",
    "    def fit(self, X, Y, Z):\n",
    "        X = self._reshape_data(X)\n",
    "        Y = self._reshape_data(Y)\n",
    "        Z = self._reshape_data(Z)\n",
    "\n",
    "        # Step 1: Linear regression of Y on Z\n",
    "        Z_ones = np.column_stack((np.ones(Z.shape[0]), Z))\n",
    "        self.beta_Z = np.linalg.pinv(Z_ones.T @ Z_ones) @ Z_ones.T @ Y\n",
    "        Y_hat = Z_ones @ self.beta_Z\n",
    "        e_Y = Y - Y_hat  # Residual of Y\n",
    "\n",
    "        # Step 2: Regress the residual on X using the included regression model\n",
    "        self.regression_model.fit(X, e_Y)\n",
    "\n",
    "        # Obtain coefficients from the regression model\n",
    "        if hasattr(self.regression_model, 'coef_'):\n",
    "            self.beta_X = self.regression_model.coef_\n",
    "        elif hasattr(self.regression_model, 'getBeta'):\n",
    "            beta_output = self.regression_model.getBeta()\n",
    "            self.beta_X = self._reshape_output(beta_output)\n",
    "\n",
    "        # Obtain negative logarithm of p-values, if available\n",
    "        if hasattr(self.regression_model, 'getNegLogP'):\n",
    "            neg_log_p_output = self.regression_model.getNegLogP()\n",
    "            if neg_log_p_output is not None:\n",
    "                self.neg_log_p_values = self._reshape_output(neg_log_p_output)\n",
    "                self.p_values = np.exp(-self.neg_log_p_values)\n",
    "                # Concatenate the p-values of Z and X. The p-values of Z were not computed, mark with NaN.\n",
    "                p_values_Z = np.full(Z.shape[1], np.nan)\n",
    "                self.p_values = np.concatenate((p_values_Z, self.p_values))\n",
    "\n",
    "    def predict(self, X, Z):\n",
    "        X = self._reshape_data(X)\n",
    "        Z = self._reshape_data(Z)\n",
    "\n",
    "        Z_ones = np.column_stack((np.ones(Z.shape[0]), Z))\n",
    "        ZX = np.column_stack((Z, X))\n",
    "        combined_beta = np.concatenate((self.beta_Z[1:].ravel(), self.beta_X.ravel()))\n",
    "        return ZX @ combined_beta + self.beta_Z[0]\n",
    "\n",
    "    def get_coefficients(self):\n",
    "        return np.concatenate((self.beta_Z[1:].ravel(), self.beta_X.ravel()))\n",
    "\n",
    "    def get_p_values(self):\n",
    "        return self.p_values\n",
    "\n",
    "    def predict(self, X, Z):\n",
    "        X = self._reshape_data(X)\n",
    "        Z = self._reshape_data(Z)\n",
    "\n",
    "        Z_ones = np.column_stack((np.ones(Z.shape[0]), Z))\n",
    "        ZX = np.column_stack((Z, X))\n",
    "        combined_beta = np.concatenate((self.beta_Z[1:].ravel(), self.beta_X.ravel()))\n",
    "        return ZX @ combined_beta + self.beta_Z[0]\n",
    "\n",
    "    def get_coefficients(self):\n",
    "        return np.concatenate((self.beta_Z[1:].ravel(), self.beta_X.ravel()))\n",
    "\n",
    "    def get_p_values(self):\n",
    "        return self.p_values\n",
    "\n",
    "\n",
    "def cross_validation(X, Y, Z, model_constructor, model_params, k=5, target_type='binary'):\n",
    "    assert target_type in ['binary', 'continuous'], \"The target type must be chosen from 'binary' or 'continuous'\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    fold_size = len(X) // k\n",
    "    performances = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Split data into train and test based on the current fold\n",
    "        test_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.setdiff1d(indices, test_indices)\n",
    "\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
    "        Z_train, Z_test = Z[train_indices], Z[test_indices]\n",
    "\n",
    "        normalized_X_train, normalized_X_test = normalize_data(X_train, X_test)\n",
    "        normalized_Z_train, normalized_Z_test = normalize_data(Z_train, Z_test)\n",
    "\n",
    "        # model = model_constructor(**model_params)\n",
    "        model = ResidualizationRegressor(model_constructor, model_params)\n",
    "        model.fit(normalized_X_train, Y_train, normalized_Z_train)\n",
    "        predictions = model.predict(normalized_X_test, normalized_Z_test)\n",
    "\n",
    "        if target_type == 'binary':\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "            Y_test = (Y_test > 0.5).astype(int)\n",
    "            performance = accuracy_score(Y_test, predictions)\n",
    "        elif target_type == 'continuous':\n",
    "            performance = mean_squared_error(Y_test, predictions)\n",
    "\n",
    "        performances.append(performance)\n",
    "\n",
    "    cv_mean = np.mean(performances)\n",
    "    cv_std = np.std(performances)\n",
    "\n",
    "    if target_type == 'binary':\n",
    "        print(f'The cross-validation accuracy is {(cv_mean * 100):.2f}% ± {(cv_std * 100):.2f}%')\n",
    "    else:\n",
    "        print(f'The cross-validation MSE is {(cv_mean * 100):.2f}% ± {(cv_std * 100):.2f}%')\n",
    "\n",
    "    return cv_mean, cv_std\n",
    "\n",
    "def interpret_result(model: Any, feature_names: List[str], trait: str, condition: str,\n",
    "                     threshold: float = 0.05, save_output: bool = True,\n",
    "                     output_dir: str = './output', model_id: int = 1) -> None:\n",
    "    \"\"\"This function interprets and reports the result of a trained linear regression model, where the regressor\n",
    "    consists of one variable about condition and multiple variables about genetic factors.\n",
    "    The function extracts coefficients and p-values from the model, and identifies the significant genes based on\n",
    "    p-values or non-zero coefficients, depending on the availability of p-values.\n",
    "\n",
    "    Parameters:\n",
    "    model (Any): The trained regression Model.\n",
    "    feature_names (List[str]): A list of feature names corresponding to the model's coefficients.\n",
    "    trait (str): The target trait of interest.\n",
    "    condition (str): The specific condition to examine within the model.\n",
    "    threshold (float): Significance level for p-value correction. Defaults to 0.05.\n",
    "    save_output (bool): Flag to determine whether to save the output to a file. Defaults to True.\n",
    "    output_dir (str): Directory path where output files are saved. Defaults to './output'.\n",
    "    model_id (int): The index of the model, 1 or 2.\n",
    "\n",
    "    Returns:\n",
    "    None: This function does not return anything but prints and optionally saves the output.\n",
    "    \"\"\"\n",
    "    coefficients = model.get_coefficients().reshape(-1).tolist()\n",
    "    p_values = model.get_p_values()\n",
    "    if p_values is None:\n",
    "        regression_df = pd.DataFrame({\n",
    "            'Variable': feature_names,\n",
    "            'Coefficient': coefficients\n",
    "        })\n",
    "    else:\n",
    "        regression_df = pd.DataFrame({\n",
    "            'Variable': feature_names,\n",
    "            'Coefficient': coefficients,\n",
    "            'p_value': p_values.reshape(-1).tolist()\n",
    "        })\n",
    "\n",
    "    condition_effect = regression_df[regression_df['Variable'] == condition].iloc[0]\n",
    "\n",
    "    print(f\"Effect of the condition on the target variable:\")\n",
    "    print(f\"Variable: {condition}\")\n",
    "    print(f\"Coefficient: {condition_effect['Coefficient']:.4f}\")\n",
    "    gene_regression_df = regression_df[regression_df['Variable'] != condition]\n",
    "    if p_values is None:\n",
    "        gene_regression_df['Absolute Coefficient'] = gene_regression_df['Coefficient'].abs()\n",
    "        significant_genes = gene_regression_df[gene_regression_df['Coefficient'] != 0]\n",
    "        significant_genes_sorted = significant_genes.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "        print(\n",
    "            f\"Found {len(significant_genes_sorted)} genes with non-zero coefficients associated with the trait '{trait}' \"\n",
    "            f\"conditional on the factor '{condition}'. These genes are identified as significant based on the regression model.\")\n",
    "    else:\n",
    "        # Apply the Benjamini-Hochberg correction, to get the corrected p-values\n",
    "        corrected_p_values = multipletests(gene_regression_df['p_value'], alpha=threshold, method='fdr_bh')[1]\n",
    "        gene_regression_df.loc[:, 'corrected_p_value'] = corrected_p_values\n",
    "        significant_genes = gene_regression_df.loc[gene_regression_df['corrected_p_value'] < threshold]\n",
    "        significant_genes_sorted = significant_genes.sort_values('corrected_p_value')\n",
    "        print(\n",
    "            f\"Found {len(significant_genes_sorted)} significant genes associated with the trait '{trait}' conditional on \"\n",
    "            f\"the factor '{condition}', with corrected p-value < {threshold}:\")\n",
    "\n",
    "    print(significant_genes_sorted.to_string(index=False))\n",
    "\n",
    "    # Optionally, save this to a CSV file\n",
    "    if save_output:\n",
    "        significant_genes_sorted.to_csv(\n",
    "            os.path.join(output_dir, f'significant_genes_condition_{condition}_{model_id}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your preferred name\n",
    "USER = \"Jake\"\n",
    "# Set the data and output directories\n",
    "DATA_ROOT = '/Users/jacob/Documents'\n",
    "OUTPUT_ROOT = '/Users/jacob/Documents/Output'\n",
    "TRAIT = 'Metabolic-Rate'\n",
    "\n",
    "OUTPUT_DIR = os.path.join(OUTPUT_ROOT, USER, '-'.join(TRAIT.split()))\n",
    "JSON_PATH = os.path.join(OUTPUT_DIR, \"cohort_info.json\")\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Gene symbol normalization may take 1-2 minutes. You may set it to False for debugging.\n",
    "NORMALIZE_GENE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data preprocessing and selection\n",
    "\n",
    "The GEO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GSE101492',\n",
       " 'GSE102177',\n",
       " 'GSE106800',\n",
       " 'GSE122279',\n",
       " 'GSE141479',\n",
       " 'GSE151683',\n",
       " 'GSE83414',\n",
       " 'GSE89231']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'GEO'\n",
    "trait_subdir = \"Metabolic-Rate\"\n",
    "\n",
    "trait_path = os.path.join(DATA_ROOT, dataset, trait_subdir)\n",
    "os.listdir(trait_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jacob/Documents\\\\GEO\\\\Metabolic-Rate\\\\GSE101492\\\\GSE101492_family.soft.gz',\n",
       " '/Users/jacob/Documents\\\\GEO\\\\Metabolic-Rate\\\\GSE101492\\\\GSE101492_series_matrix.txt.gz')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort = accession_num = \"GSE101492\"\n",
    "cohort_dir = os.path.join(trait_path, accession_num)\n",
    "soft_file, matrix_file = get_relevant_filepaths(cohort_dir)\n",
    "soft_file, matrix_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!Series_title\t\"LONG NON-CODING RNAs ASSOCIATED WITH METABOLIC TRAITS IN HUMAN WHITE ADIPOSE TISSUE\"\n",
      "!Series_summary\t\"The aim of this study was to identify novel long noncoding RNAs (lncRNAs) that are differentially expressed in the subcutaneous region either in obesity or insulin resistance.\"\n",
      "!Series_summary\t\"A number of novel lncRNAs were differentially expressed in the subcutaneous region either in obesity or insulin resistance. Two lncRNAs (termed here as adipocyte specific insulin resistance related lncRNAs (ADLNRIs), ADLNRI 1 and ADLNRI 2) were influenced by both conditions and enriched in adipocytes. Knockdown of either by antisense oligonucleotides in fat cellsadipocytes altered lipolysis and adiponectin secretion, respectively.\"\n",
      "!Series_overall_design\t\"The 80 women included in this study were selected from the extremes of insulin sensitivity as measured by homeostasis model assessment of IR (HOMAIR) among 220 obese women who participated in a clinical trial on the effect of bariatric surgery (NCT01785134 at www.clinicaltrials.gov). All sampling and measurements were performed before or during bariatric surgery (laparoscopic gastric bypass). Participants were investigated at 8 AM after an overnight fast. Insulin sensitivity was assessed by HOMAIR and was calculated from fasting measures of glucose and insulin. High HOMAIR values indicate IR. The 40 women with the highest HOMAIR values and the 40 women with the lowest values were selected for inclusion in the present study.  Differential gene expression analysis was conducted at the gene level using Limma, selecting for genes with a false discovery rate of <0.05\"\n"
     ]
    }
   ],
   "source": [
    "background_prefixes = ['!Series_title', '!Series_summary', '!Series_overall_design']\n",
    "clinical_prefixes = ['!Sample_geo_accession', '!Sample_characteristics_ch1']\n",
    "\n",
    "background_info, clinical_data = get_background_and_clinical_data(matrix_file, background_prefixes, clinical_prefixes)\n",
    "print(background_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!Sample_geo_accession</th>\n",
       "      <th>GSM2704900</th>\n",
       "      <th>GSM2704901</th>\n",
       "      <th>GSM2704902</th>\n",
       "      <th>GSM2704903</th>\n",
       "      <th>GSM2704904</th>\n",
       "      <th>GSM2704905</th>\n",
       "      <th>GSM2704906</th>\n",
       "      <th>GSM2704907</th>\n",
       "      <th>GSM2704908</th>\n",
       "      <th>...</th>\n",
       "      <th>GSM2704970</th>\n",
       "      <th>GSM2704971</th>\n",
       "      <th>GSM2704972</th>\n",
       "      <th>GSM2704973</th>\n",
       "      <th>GSM2704974</th>\n",
       "      <th>GSM2704975</th>\n",
       "      <th>GSM2704976</th>\n",
       "      <th>GSM2704977</th>\n",
       "      <th>GSM2704978</th>\n",
       "      <th>GSM2704979</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!Sample_characteristics_ch1</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>...</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "      <td>suject status: obese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Sample_characteristics_ch1</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>...</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "      <td>gender: female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!Sample_characteristics_ch1</td>\n",
       "      <td>age: 39</td>\n",
       "      <td>age: 28</td>\n",
       "      <td>age: 42</td>\n",
       "      <td>age: 30</td>\n",
       "      <td>age: 42</td>\n",
       "      <td>age: 37</td>\n",
       "      <td>age: 36</td>\n",
       "      <td>age: 33</td>\n",
       "      <td>age: 27</td>\n",
       "      <td>...</td>\n",
       "      <td>age: 33</td>\n",
       "      <td>age: 32</td>\n",
       "      <td>age: 38</td>\n",
       "      <td>age: 31</td>\n",
       "      <td>age: 30</td>\n",
       "      <td>age: 28</td>\n",
       "      <td>age: 27</td>\n",
       "      <td>age: 45</td>\n",
       "      <td>age: 40</td>\n",
       "      <td>age: 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!Sample_characteristics_ch1</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>...</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin sensitive</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "      <td>insulin sensitivity: insulin resistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!Sample_characteristics_ch1</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>...</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "      <td>tissue: subcutaneous white adipose tissue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         !Sample_geo_accession                                 GSM2704900  \\\n",
       "0  !Sample_characteristics_ch1                       suject status: obese   \n",
       "1  !Sample_characteristics_ch1                             gender: female   \n",
       "2  !Sample_characteristics_ch1                                    age: 39   \n",
       "3  !Sample_characteristics_ch1     insulin sensitivity: insulin sensitive   \n",
       "4  !Sample_characteristics_ch1  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704901  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 28   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704902  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 42   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704903  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 30   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704904  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 42   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704905  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 37   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704906  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 36   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704907  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 33   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704908  ...  \\\n",
       "0                       suject status: obese  ...   \n",
       "1                             gender: female  ...   \n",
       "2                                    age: 27  ...   \n",
       "3     insulin sensitivity: insulin sensitive  ...   \n",
       "4  tissue: subcutaneous white adipose tissue  ...   \n",
       "\n",
       "                                  GSM2704970  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 33   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704971  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 32   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704972  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 38   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704973  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 31   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704974  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 30   \n",
       "3     insulin sensitivity: insulin sensitive   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704975  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 28   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704976  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 27   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704977  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 45   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704978  \\\n",
       "0                       suject status: obese   \n",
       "1                             gender: female   \n",
       "2                                    age: 40   \n",
       "3     insulin sensitivity: insulin resistant   \n",
       "4  tissue: subcutaneous white adipose tissue   \n",
       "\n",
       "                                  GSM2704979  \n",
       "0                       suject status: obese  \n",
       "1                             gender: female  \n",
       "2                                    age: 25  \n",
       "3     insulin sensitivity: insulin resistant  \n",
       "4  tissue: subcutaneous white adipose tissue  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['suject status: obese'],\n",
       " 1: ['gender: female'],\n",
       " 2: ['age: 39',\n",
       "  'age: 28',\n",
       "  'age: 42',\n",
       "  'age: 30',\n",
       "  'age: 37',\n",
       "  'age: 36',\n",
       "  'age: 33',\n",
       "  'age: 27',\n",
       "  'age: 43',\n",
       "  'age: 44',\n",
       "  'age: 25',\n",
       "  'age: 35',\n",
       "  'age: 40',\n",
       "  'age: 29',\n",
       "  'age: 41',\n",
       "  'age: 34',\n",
       "  'age: 31',\n",
       "  'age: 38',\n",
       "  'age: 26',\n",
       "  'age: 32',\n",
       "  'age: 45'],\n",
       " 3: ['insulin sensitivity: insulin sensitive',\n",
       "  'insulin sensitivity: insulin resistant'],\n",
       " 4: ['tissue: subcutaneous white adipose tissue']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_data_unique = get_unique_values_by_row(clinical_data)\n",
    "clinical_data_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a biomedical research team, we are selecting datasets to study the association between the human trait \\'Metabolic-Rate\\' and genetic factors, optionally considering the influence of age and gender. After searching the GEO database and parsing the matrix file of a series, we obtained background information and sample characteristics data. We will provide textual information about the dataset background, and a Python dictionary storing a list of unique values for each field of the sample characteristics data. Please carefully review the provided information and answer the following questions about this dataset:\\n1. Does this dataset contain gene expression data? (Note: Pure miRNA data is not suitable.)\\n2. For each of the traits \\'Metabolic-Rate\\', \\'age\\', and \\'gender\\', please address these points:\\n   (1) Is there human data available for this trait?\\n   (2) If so, identify the key in the sample characteristics dictionary where unique values of this trait is recorded. The key is an integer. The trait information might be explicitly recorded, or can be inferred from the field with some biomedical knowledge or understanding about the data collection process.\\n   (3) Choose an appropriate data type (either \\'continuous\\' or \\'binary\\') for each trait. Write a Python function to convert any given value of the trait to this data type. The function should handle inference about the trait value and convert unknown values to None.\\n   Name the functions \\'convert_trait\\', \\'convert_age\\', and \\'convert_gender\\', respectively.\\n\\nBackground information about the dataset:\\n!Series_title\\t\"LONG NON-CODING RNAs ASSOCIATED WITH METABOLIC TRAITS IN HUMAN WHITE ADIPOSE TISSUE\"\\n!Series_summary\\t\"The aim of this study was to identify novel long noncoding RNAs (lncRNAs) that are differentially expressed in the subcutaneous region either in obesity or insulin resistance.\"\\n!Series_summary\\t\"A number of novel lncRNAs were differentially expressed in the subcutaneous region either in obesity or insulin resistance. Two lncRNAs (termed here as adipocyte specific insulin resistance related lncRNAs (ADLNRIs), ADLNRI 1 and ADLNRI 2) were influenced by both conditions and enriched in adipocytes. Knockdown of either by antisense oligonucleotides in fat cellsadipocytes altered lipolysis and adiponectin secretion, respectively.\"\\n!Series_overall_design\\t\"The 80 women included in this study were selected from the extremes of insulin sensitivity as measured by homeostasis model assessment of IR (HOMAIR) among 220 obese women who participated in a clinical trial on the effect of bariatric surgery (NCT01785134 at www.clinicaltrials.gov). All sampling and measurements were performed before or during bariatric surgery (laparoscopic gastric bypass). Participants were investigated at 8 AM after an overnight fast. Insulin sensitivity was assessed by HOMAIR and was calculated from fasting measures of glucose and insulin. High HOMAIR values indicate IR. The 40 women with the highest HOMAIR values and the 40 women with the lowest values were selected for inclusion in the present study.  Differential gene expression analysis was conducted at the gene level using Limma, selecting for genes with a false discovery rate of <0.05\"\\n\\nSample characteristics dictionary (from \"!Sample_characteristics_ch1\", converted to a Python dictionary that stores the unique values for each field):\\n{0: [\\'suject status: obese\\'], 1: [\\'gender: female\\'], 2: [\\'age: 39\\', \\'age: 28\\', \\'age: 42\\', \\'age: 30\\', \\'age: 37\\', \\'age: 36\\', \\'age: 33\\', \\'age: 27\\', \\'age: 43\\', \\'age: 44\\', \\'age: 25\\', \\'age: 35\\', \\'age: 40\\', \\'age: 29\\', \\'age: 41\\', \\'age: 34\\', \\'age: 31\\', \\'age: 38\\', \\'age: 26\\', \\'age: 32\\', \\'age: 45\\'], 3: [\\'insulin sensitivity: insulin sensitive\\', \\'insulin sensitivity: insulin resistant\\'], 4: [\\'tissue: subcutaneous white adipose tissue\\']}\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f'''As a biomedical research team, we are selecting datasets to study the association between the human trait \\'{TRAIT}\\' and genetic factors, optionally considering the influence of age and gender. After searching the GEO database and parsing the matrix file of a series, we obtained background information and sample characteristics data. We will provide textual information about the dataset background, and a Python dictionary storing a list of unique values for each field of the sample characteristics data. Please carefully review the provided information and answer the following questions about this dataset:\n",
    "1. Does this dataset contain gene expression data? (Note: Pure miRNA data is not suitable.)\n",
    "2. For each of the traits \\'{TRAIT}\\', 'age', and 'gender', please address these points:\n",
    "   (1) Is there human data available for this trait?\n",
    "   (2) If so, identify the key in the sample characteristics dictionary where unique values of this trait is recorded. The key is an integer. The trait information might be explicitly recorded, or can be inferred from the field with some biomedical knowledge or understanding about the data collection process.\n",
    "   (3) Choose an appropriate data type (either 'continuous' or 'binary') for each trait. Write a Python function to convert any given value of the trait to this data type. The function should handle inference about the trait value and convert unknown values to None.\n",
    "   Name the functions 'convert_trait', 'convert_age', and 'convert_gender', respectively.\n",
    "\n",
    "Background information about the dataset:\n",
    "{background_info}\n",
    "\n",
    "Sample characteristics dictionary (from \"!Sample_characteristics_ch1\", converted to a Python dictionary that stores the unique values for each field):\n",
    "{clinical_data_unique}\n",
    "'''\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gene_availabe = True\n",
    "trait_row = 3\n",
    "age_row = 2\n",
    "gender_row = 1\n",
    "\n",
    "trait_type = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_available = is_gene_availabe and (trait_row is not None)\n",
    "if not is_available:\n",
    "    save_cohort_info(cohort, JSON_PATH, is_available)\n",
    "    print(\"This cohort is not usable. Please skip the following steps and jump to the next accession number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and use the functions generated by GPT\n",
    "\n",
    "def convert_trait(trait_string):\n",
    "    \"\"\"\n",
    "    Convert insulin sensitivity string to a binary value.\n",
    "    'insulin resistant' is represented as 1, 'insulin sensitive' as 0.\n",
    "    Unknown or other values are converted to None.\n",
    "    \"\"\"\n",
    "    if trait_string.lower() == 'insulin sensitivity: insulin resistant':\n",
    "        return 1\n",
    "    elif trait_string.lower() == 'insulin sensitivity: insulin sensitive':\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_age(age_string):\n",
    "    \"\"\"\n",
    "    Convert age string to a continuous numerical value.\n",
    "    Unknown values are converted to None.\n",
    "    \"\"\"\n",
    "    if age_string.lower() == 'n.a.':\n",
    "        return None\n",
    "    try:\n",
    "        # Extract age as an integer from the string\n",
    "        age = int(age_string.split(': ')[1])\n",
    "        return age\n",
    "    except (ValueError, IndexError):\n",
    "        # In case of any format error or unexpected string structure\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# It sometimes maps 'female' to 0, and sometimes 1. Does it matter?\n",
    "# Regarding the mapping of 'female' to 0 or 1, it doesn't matter as long as it's consistent throughout the dataset and its interpretation. It's just a matter of convention, and as long as it's documented properly, it shouldn't cause any issues.\n",
    "\n",
    "\n",
    "def convert_gender(gender_string):\n",
    "    \"\"\"\n",
    "    Convert gender string to a binary value.\n",
    "    'female' is represented as 1, 'male' as 0.\n",
    "    Unknown values are converted to None.\n",
    "    \"\"\"\n",
    "    if gender_string.lower() == 'gender: female':\n",
    "        return 1\n",
    "    elif gender_string.lower() == 'gender: male':\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_18040\\4263505135.py:344: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  clinical_df = clinical_df.applymap(convert_fn)\n",
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_18040\\4263505135.py:344: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  clinical_df = clinical_df.applymap(convert_fn)\n",
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_18040\\4263505135.py:344: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  clinical_df = clinical_df.applymap(convert_fn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GSM2704900</th>\n",
       "      <th>GSM2704901</th>\n",
       "      <th>GSM2704902</th>\n",
       "      <th>GSM2704903</th>\n",
       "      <th>GSM2704904</th>\n",
       "      <th>GSM2704905</th>\n",
       "      <th>GSM2704906</th>\n",
       "      <th>GSM2704907</th>\n",
       "      <th>GSM2704908</th>\n",
       "      <th>GSM2704909</th>\n",
       "      <th>...</th>\n",
       "      <th>GSM2704970</th>\n",
       "      <th>GSM2704971</th>\n",
       "      <th>GSM2704972</th>\n",
       "      <th>GSM2704973</th>\n",
       "      <th>GSM2704974</th>\n",
       "      <th>GSM2704975</th>\n",
       "      <th>GSM2704976</th>\n",
       "      <th>GSM2704977</th>\n",
       "      <th>GSM2704978</th>\n",
       "      <th>GSM2704979</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Metabolic-Rate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                GSM2704900  GSM2704901  GSM2704902  GSM2704903  GSM2704904  \\\n",
       "Metabolic-Rate           0           0           1           0           0   \n",
       "Age                     39          28          42          30          42   \n",
       "Gender                   1           1           1           1           1   \n",
       "\n",
       "                GSM2704905  GSM2704906  GSM2704907  GSM2704908  GSM2704909  \\\n",
       "Metabolic-Rate           0           1           0           0           0   \n",
       "Age                     37          36          33          27          43   \n",
       "Gender                   1           1           1           1           1   \n",
       "\n",
       "                ...  GSM2704970  GSM2704971  GSM2704972  GSM2704973  \\\n",
       "Metabolic-Rate  ...           1           1           0           1   \n",
       "Age             ...          33          32          38          31   \n",
       "Gender          ...           1           1           1           1   \n",
       "\n",
       "                GSM2704974  GSM2704975  GSM2704976  GSM2704977  GSM2704978  \\\n",
       "Metabolic-Rate           0           1           1           1           1   \n",
       "Age                     30          28          27          45          40   \n",
       "Gender                   1           1           1           1           1   \n",
       "\n",
       "                GSM2704979  \n",
       "Metabolic-Rate           1  \n",
       "Age                     25  \n",
       "Gender                   1  \n",
       "\n",
       "[3 rows x 80 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_clinical_data = geo_select_clinical_features(clinical_data, TRAIT, trait_row, convert_trait, age_row=age_row,\n",
    "                                                      convert_age=convert_age, gender_row=gender_row,\n",
    "                                                      convert_gender=convert_gender)\n",
    "selected_clinical_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GSM2704900</th>\n",
       "      <th>GSM2704901</th>\n",
       "      <th>GSM2704902</th>\n",
       "      <th>GSM2704903</th>\n",
       "      <th>GSM2704904</th>\n",
       "      <th>GSM2704905</th>\n",
       "      <th>GSM2704906</th>\n",
       "      <th>GSM2704907</th>\n",
       "      <th>GSM2704908</th>\n",
       "      <th>GSM2704909</th>\n",
       "      <th>...</th>\n",
       "      <th>GSM2704970</th>\n",
       "      <th>GSM2704971</th>\n",
       "      <th>GSM2704972</th>\n",
       "      <th>GSM2704973</th>\n",
       "      <th>GSM2704974</th>\n",
       "      <th>GSM2704975</th>\n",
       "      <th>GSM2704976</th>\n",
       "      <th>GSM2704977</th>\n",
       "      <th>GSM2704978</th>\n",
       "      <th>GSM2704979</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18670005</th>\n",
       "      <td>12.9053</td>\n",
       "      <td>12.7845</td>\n",
       "      <td>12.9755</td>\n",
       "      <td>12.8532</td>\n",
       "      <td>12.9113</td>\n",
       "      <td>12.8426</td>\n",
       "      <td>12.9506</td>\n",
       "      <td>13.1457</td>\n",
       "      <td>12.9220</td>\n",
       "      <td>12.8854</td>\n",
       "      <td>...</td>\n",
       "      <td>12.6514</td>\n",
       "      <td>12.5237</td>\n",
       "      <td>12.5239</td>\n",
       "      <td>12.4522</td>\n",
       "      <td>12.7518</td>\n",
       "      <td>12.7113</td>\n",
       "      <td>12.7256</td>\n",
       "      <td>12.7989</td>\n",
       "      <td>12.6480</td>\n",
       "      <td>12.8128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18670007</th>\n",
       "      <td>12.3988</td>\n",
       "      <td>12.2140</td>\n",
       "      <td>12.1375</td>\n",
       "      <td>12.1021</td>\n",
       "      <td>12.0092</td>\n",
       "      <td>12.2770</td>\n",
       "      <td>12.2382</td>\n",
       "      <td>12.5264</td>\n",
       "      <td>12.4820</td>\n",
       "      <td>12.1760</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8926</td>\n",
       "      <td>11.8789</td>\n",
       "      <td>11.8802</td>\n",
       "      <td>11.8450</td>\n",
       "      <td>12.1582</td>\n",
       "      <td>12.0323</td>\n",
       "      <td>11.8715</td>\n",
       "      <td>12.1235</td>\n",
       "      <td>11.8930</td>\n",
       "      <td>11.9827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18670009</th>\n",
       "      <td>12.2866</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>12.4862</td>\n",
       "      <td>12.2454</td>\n",
       "      <td>12.4239</td>\n",
       "      <td>12.4078</td>\n",
       "      <td>12.4023</td>\n",
       "      <td>12.6458</td>\n",
       "      <td>12.4636</td>\n",
       "      <td>12.2331</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0701</td>\n",
       "      <td>12.0844</td>\n",
       "      <td>12.1345</td>\n",
       "      <td>11.8740</td>\n",
       "      <td>12.3501</td>\n",
       "      <td>12.1469</td>\n",
       "      <td>12.3626</td>\n",
       "      <td>12.3413</td>\n",
       "      <td>12.1106</td>\n",
       "      <td>12.2803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18670011</th>\n",
       "      <td>12.3424</td>\n",
       "      <td>12.3969</td>\n",
       "      <td>12.3662</td>\n",
       "      <td>12.0945</td>\n",
       "      <td>12.2141</td>\n",
       "      <td>12.4370</td>\n",
       "      <td>12.3977</td>\n",
       "      <td>12.6638</td>\n",
       "      <td>12.5197</td>\n",
       "      <td>12.3425</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7843</td>\n",
       "      <td>12.0105</td>\n",
       "      <td>11.6471</td>\n",
       "      <td>11.7735</td>\n",
       "      <td>12.0842</td>\n",
       "      <td>11.8684</td>\n",
       "      <td>11.9962</td>\n",
       "      <td>12.1834</td>\n",
       "      <td>11.9810</td>\n",
       "      <td>11.9306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18670020</th>\n",
       "      <td>10.6459</td>\n",
       "      <td>10.8770</td>\n",
       "      <td>10.9622</td>\n",
       "      <td>10.8599</td>\n",
       "      <td>11.0323</td>\n",
       "      <td>11.1980</td>\n",
       "      <td>10.8373</td>\n",
       "      <td>11.4471</td>\n",
       "      <td>10.6688</td>\n",
       "      <td>10.6547</td>\n",
       "      <td>...</td>\n",
       "      <td>10.7551</td>\n",
       "      <td>10.3526</td>\n",
       "      <td>10.1683</td>\n",
       "      <td>10.4271</td>\n",
       "      <td>10.4775</td>\n",
       "      <td>10.3934</td>\n",
       "      <td>10.8407</td>\n",
       "      <td>10.3070</td>\n",
       "      <td>10.1430</td>\n",
       "      <td>10.6485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GSM2704900  GSM2704901  GSM2704902  GSM2704903  GSM2704904  \\\n",
       "ID                                                                     \n",
       "18670005     12.9053     12.7845     12.9755     12.8532     12.9113   \n",
       "18670007     12.3988     12.2140     12.1375     12.1021     12.0092   \n",
       "18670009     12.2866     12.3892     12.4862     12.2454     12.4239   \n",
       "18670011     12.3424     12.3969     12.3662     12.0945     12.2141   \n",
       "18670020     10.6459     10.8770     10.9622     10.8599     11.0323   \n",
       "\n",
       "          GSM2704905  GSM2704906  GSM2704907  GSM2704908  GSM2704909  ...  \\\n",
       "ID                                                                    ...   \n",
       "18670005     12.8426     12.9506     13.1457     12.9220     12.8854  ...   \n",
       "18670007     12.2770     12.2382     12.5264     12.4820     12.1760  ...   \n",
       "18670009     12.4078     12.4023     12.6458     12.4636     12.2331  ...   \n",
       "18670011     12.4370     12.3977     12.6638     12.5197     12.3425  ...   \n",
       "18670020     11.1980     10.8373     11.4471     10.6688     10.6547  ...   \n",
       "\n",
       "          GSM2704970  GSM2704971  GSM2704972  GSM2704973  GSM2704974  \\\n",
       "ID                                                                     \n",
       "18670005     12.6514     12.5237     12.5239     12.4522     12.7518   \n",
       "18670007     11.8926     11.8789     11.8802     11.8450     12.1582   \n",
       "18670009     12.0701     12.0844     12.1345     11.8740     12.3501   \n",
       "18670011     11.7843     12.0105     11.6471     11.7735     12.0842   \n",
       "18670020     10.7551     10.3526     10.1683     10.4271     10.4775   \n",
       "\n",
       "          GSM2704975  GSM2704976  GSM2704977  GSM2704978  GSM2704979  \n",
       "ID                                                                    \n",
       "18670005     12.7113     12.7256     12.7989     12.6480     12.8128  \n",
       "18670007     12.0323     11.8715     12.1235     11.8930     11.9827  \n",
       "18670009     12.1469     12.3626     12.3413     12.1106     12.2803  \n",
       "18670011     11.8684     11.9962     12.1834     11.9810     11.9306  \n",
       "18670020     10.3934     10.8407     10.3070     10.1430     10.6485  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genetic_data = get_genetic_data(matrix_file)\n",
    "genetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18670005',\n",
       " '18670007',\n",
       " '18670009',\n",
       " '18670011',\n",
       " '18670020',\n",
       " '18670022',\n",
       " '18670023',\n",
       " '18670027',\n",
       " '18670028',\n",
       " '18670032',\n",
       " '18670033',\n",
       " '18670036',\n",
       " '18670038',\n",
       " '18670039',\n",
       " '18670041',\n",
       " '18670049',\n",
       " '18670051',\n",
       " '18670053',\n",
       " '18670054',\n",
       " '18670056']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_row_ids = genetic_data.index[:20].tolist()\n",
    "gene_row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBelow are the row headers of a gene expression dataset in GEO. Based on your biomedical knowledge, are they human gene symbols, or are they some other identifiers that need to be mapped to gene symbols? Your answer should be concluded by starting a new line and strictly following this format:\\nrequires_gene_mapping = (True or False)\\n\\nRow headers:\\n['18670005', '18670007', '18670009', '18670011', '18670020', '18670022', '18670023', '18670027', '18670028', '18670032', '18670033', '18670036', '18670038', '18670039', '18670041', '18670049', '18670051', '18670053', '18670054', '18670056']\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f'''\n",
    "Below are the row headers of a gene expression dataset in GEO. Based on your biomedical knowledge, are they human gene symbols, or are they some other identifiers that need to be mapped to gene symbols? Your answer should be concluded by starting a new line and strictly following this format:\n",
    "requires_gene_mapping = (True or False)\n",
    "\n",
    "Row headers:\n",
    "{gene_row_ids}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_gene_mapping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': ['18670005', '18670007', '18670009', '18670011', '18670020'], 'probeset_id': ['18670005', '18670007', '18670009', '18670011', '18670020'], 'seqname': ['---', '---', '---', '---', '---'], 'strand': ['---', '---', '---', '---', '---'], 'start': ['---', '---', '---', '---', '---'], 'stop': ['---', '---', '---', '---', '---'], 'total_probes': [3.0, 1.0, 2.0, 2.0, 2.0], 'gene_assignment': ['--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---'], 'mrna_assignment': ['---', '---', '---', '---', '---'], 'swissprot': ['---', '---', '---', '---', '---'], 'unigene': ['---', '---', '---', '---', '---'], 'category': ['normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon'], 'locus type': ['normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon'], 'notes': ['---', '---', '---', '---', '---'], 'SPOT_ID': ['normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon']}\n"
     ]
    }
   ],
   "source": [
    "if requires_gene_mapping:\n",
    "    gene_annotation = get_gene_annotation(soft_file)\n",
    "    gene_annotation_summary = preview_df(gene_annotation)\n",
    "    print(gene_annotation_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    As a biomedical research team, we are analyzing a gene expression dataset, and find that its row headers are some identifiers related to genes:\n",
      "    ['18670005', '18670007', '18670009', '18670011', '18670020', '18670022', '18670023', '18670027', '18670028', '18670032', '18670033', '18670036', '18670038', '18670039', '18670041', '18670049', '18670051', '18670053', '18670054', '18670056']\n",
      "    To get the mapping from those identifiers to actual gene symbols, we extracted the gene annotation data from a series in the GEO database, and saved it to a Python dictionary. Please read the dictionary, and decide which key stores the identifiers, and which key stores the gene symbols. Please strictly follow this format in your answer:\n",
      "    identifier_key = 'key_name1'\n",
      "    gene_symbol_key = 'key_name2'\n",
      "\n",
      "    Gene annotation dictionary:\n",
      "    {'ID': ['18670005', '18670007', '18670009', '18670011', '18670020'], 'probeset_id': ['18670005', '18670007', '18670009', '18670011', '18670020'], 'seqname': ['---', '---', '---', '---', '---'], 'strand': ['---', '---', '---', '---', '---'], 'start': ['---', '---', '---', '---', '---'], 'stop': ['---', '---', '---', '---', '---'], 'total_probes': [3.0, 1.0, 2.0, 2.0, 2.0], 'gene_assignment': ['--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---', '--- // --- // Positive Housekeeping Controls - Probeset Type: normgene->exon // --- // ---'], 'mrna_assignment': ['---', '---', '---', '---', '---'], 'swissprot': ['---', '---', '---', '---', '---'], 'unigene': ['---', '---', '---', '---', '---'], 'category': ['normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon'], 'locus type': ['normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon'], 'notes': ['---', '---', '---', '---', '---'], 'SPOT_ID': ['normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon', 'normgene->exon']}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if requires_gene_mapping:\n",
    "    print(f'''\n",
    "    As a biomedical research team, we are analyzing a gene expression dataset, and find that its row headers are some identifiers related to genes:\n",
    "    {gene_row_ids}\n",
    "    To get the mapping from those identifiers to actual gene symbols, we extracted the gene annotation data from a series in the GEO database, and saved it to a Python dictionary. Please read the dictionary, and decide which key stores the identifiers, and which key stores the gene symbols. Please strictly follow this format in your answer:\n",
    "    identifier_key = 'key_name1'\n",
    "    gene_symbol_key = 'key_name2'\n",
    "\n",
    "    Gene annotation dictionary:\n",
    "    {gene_annotation_summary}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if requires_gene_mapping:\n",
    "    identifier_key = 'ID'\n",
    "    gene_symbol_key = 'gene_assignment'\n",
    "    gene_mapping = get_gene_mapping(gene_annotation, identifier_key, gene_symbol_key)\n",
    "    genetic_data = apply_gene_mapping(genetic_data, gene_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "998 input query terms found no hit:\t['locus=scaffold1174:29274:29561:+ gene:ENSG00000179066', 'locus=scaffold2987:101738:101991:+ gene:E\n",
      "1000 input query terms found no hit:\t['100287205', '100287216', '100287223', '100287225', '100287225', '100287226', '100287226', '1002872\n",
      "1000 input query terms found no hit:\t['100505902', '100505903', '100505912', '100505918', '100505920', '100505921', '100505929', '1005059\n",
      "1000 input query terms found no hit:\t['100616482', '100616483', '100616484', '100616486', '100616487', '100616489', '100616490', '1006164\n",
      "1000 input query terms found no hit:\t['10436', '10437', '10437', '10438', '10439', '1044', '10440', '10443', '10444', '10445', '10445', '\n",
      "1000 input query terms found no hit:\t['11259', '112597', '11260', '11260', '112609', '11261', '112611', '112616', '11262', '11264', '1126\n",
      "1000 input query terms found no hit:\t['124817', '124842', '124857', '124871', '124872', '124912', '124923', '124925', '124925', '124930',\n",
      "1000 input query terms found no hit:\t['140890', '140893', '140894', '1409', '140901', '140902', '140947', '140kDa', '141', '1410', '1410'\n",
      "1000 input query terms found no hit:\t['157556', '157567', '157570', '157574', '1576', '157627', '157638', '157657', '157680', '157693', '\n",
      "1000 input query terms found no hit:\t['190kDa', '191', '1910', '1911', '1911', '1912', '1915', '191585', '1917', '192111', '192111', '192\n",
      "1000 input query terms found no hit:\t['219670', '219681', '219699', '2197', '2197', '219731', '219736', '219738', '219743', '219749', '21\n",
      "1000 input query terms found no hit:\t['23360', '23360', '23361', '23361', '23362', '23363', '23365', '23365', '23366', '23367', '23368', \n",
      "1000 input query terms found no hit:\t['25930', '259307', '259307', '259308', '25932', '25934', '25936', '25937', '25938', '25938', '25939\n",
      "1000 input query terms found no hit:\t['2813', '2814', '2815', '2817', '2819', '2819', '2820', '2821', '2821', '2822', '28227', '28227', '\n",
      "1000 input query terms found no hit:\t['29097', '29098', '29098', '29099', '29099', '291', '29100', '29100', '29101', '29102', '29103', '2\n",
      "1000 input query terms found no hit:\t['338661', '338661', '338662', '338662', '338674', '338674', '338675', '338675', '338692', '338699',\n",
      "1000 input query terms found no hit:\t['375061', '3751', '375133', '375189', '375190', '375196', '3752', '375248', '375260', '375287', '37\n",
      "1000 input query terms found no hit:\t['399833', '399833', '399844', '399876', '399876', '399881', '399881', '399886', '399886', '399888',\n",
      "1000 input query terms found no hit:\t['4133', '4134', '4135', '4135', '4137', '4137', '4139', '414', '4140', '4140', '414059', '414060', \n",
      "1000 input query terms found no hit:\t['4795', '4796', '4798', '4798', '4799', '47kDa', '48', '480', '4800', '4801', '4801', '4802', '4803\n",
      "1000 input query terms found no hit:\t['5134', '51340', '51341', '51341', '51343', '51343', '51347', '51347', '51348', '51348', '51350', '\n",
      "1000 input query terms found no hit:\t['54797', '54798', '54799', '5480', '54800', '54801', '54802', '54805', '54806', '54807', '54807', '\n",
      "1000 input query terms found no hit:\t['55766', '55768', '55769', '55769', '5577', '55770', '55771', '55771', '55773', '55775', '55775', '\n",
      "1000 input query terms found no hit:\t['574487', '574487', '574488', '574488', '574489', '574489', '57449', '574490', '574490', '574491', \n",
      "1000 input query terms found no hit:\t['5S ribosomal pseudogene 288', '5S ribosomal pseudogene 289', '5S ribosomal pseudogene 29', '5S rib\n",
      "1000 input query terms found no hit:\t['64084', '64087', '64089', '64090', '64091', '64092', '64093', '64093', '64094', '64096', '64097', \n",
      "1000 input query terms found no hit:\t['653505', '653509', '653513', '653519', '653543', '653544', '653545', '653548', '653550', '653566',\n",
      "1000 input query terms found no hit:\t['6948', '6949', '695', '6950', '6953', '6953', '6954', '696', '6975', '6988', '699', '6990', '6991'\n",
      "1000 input query terms found no hit:\t['7428', '7429', '7430', '7431', '7432', '7433', '7434', '7436', '7439', '7439', '744', '744', '7441\n",
      "1000 input query terms found no hit:\t['79837', '79838', '79839', '7984', '79840', '79841', '79841', '79842', '79842', '79843', '79844', '\n",
      "1000 input query terms found no hit:\t['83449', '83449', '8345', '83450', '83451', '83452', '8346', '83460', '83460', '83461', '83461', '8\n",
      "1000 input query terms found no hit:\t['84993', '84996', '8500', '8500', '85001', '85001', '85002', '85002', '85004', '85004', '85007', '8\n",
      "1000 input query terms found no hit:\t['90834', '90835', '90835', '9084', '90843', '9085', '90850', '90850', '90853', '9086', '90861', '90\n",
      "9 input query terms found dup hits:\t[('A2ML1-AS1', 2), ('A2ML1-AS2', 2), ('A2MP1', 2), ('AACSP1', 2), ('ABCA11P', 2), ('ABCA17P', 2), ('\n",
      "876 input query terms found no hit:\t['9635', '9636', '9637', '9638', '9638', '9639', '9640', '9640', '9641', '9643', '9644', '9645', '96\n",
      "1000 input query terms found no hit:\t['AC007131.2', 'AC007161.5', 'AC007163.3', 'AC007163.6', 'AC007179.1', 'AC007182.6', 'AC007246.3', '\n",
      "16 input query terms found dup hits:\t[('ACAP2-IT1', 2), ('ACTG1P4', 2), ('ACTR3BP2', 2), ('ACTR3BP4', 2), ('ACTR3BP5', 3), ('ADAM1A', 2),\n",
      "643 input query terms found no hit:\t['AC099850.1', 'AC100802.3', 'AC100848.1', 'AC102948.2', 'AC102953.4', 'AC102953.6', 'AC103563.8', '\n",
      "60 input query terms found dup hits:\t[('AK4P3', 2), ('AKR1B1P6', 2), ('AKR1C6P', 2), ('AKR7A2P1', 3), ('ALG1', 2), ('ALG10', 2), ('ALG10B\n",
      "397 input query terms found no hit:\t['AK130759', 'AK130852', 'AK130904', 'AK131021', 'AK131084', 'AK131224', 'AK131521', 'AK225211', 'AK\n",
      "24 input query terms found dup hits:\t[('ARMC2-AS1', 2), ('ARMCX3-AS1', 2), ('ARMCX5-GPRASP2', 2), ('ASB9P1', 2), ('ASH1L-IT1', 2), ('ASMT\n",
      "571 input query terms found no hit:\t['ARL2-SNX15 readthrough', 'ARL5B antisense RNA 1', 'ARL5B-AS1', 'ARMC2 antisense RNA 1', 'ARMC4', '\n",
      "1000 input query terms found no hit:\t['BC000251', 'BC000252', 'BC000253', 'BC000254', 'BC000255', 'BC000258', 'BC000259', 'BC000260', 'BC\n",
      "1000 input query terms found no hit:\t['BC001788', 'BC001789', 'BC001791', 'BC001793', 'BC001795', 'BC001796', 'BC001798', 'BC001800', 'BC\n",
      "1000 input query terms found no hit:\t['BC004427', 'BC004431', 'BC004434', 'BC004438', 'BC004439', 'BC004440', 'BC004441', 'BC004442', 'BC\n",
      "1000 input query terms found no hit:\t['BC007502', 'BC007504', 'BC007505', 'BC007506', 'BC007507', 'BC007508', 'BC007509', 'BC007512', 'BC\n",
      "1000 input query terms found no hit:\t['BC009842', 'BC009846', 'BC009848', 'BC009849', 'BC009850', 'BC009855', 'BC009858', 'BC009861', 'BC\n",
      "1000 input query terms found no hit:\t['BC012605', 'BC012606', 'BC012607', 'BC012609', 'BC012611', 'BC012612', 'BC012613', 'BC012616', 'BC\n",
      "1000 input query terms found no hit:\t['BC015547', 'BC015548', 'BC015555', 'BC015557', 'BC015558', 'BC015563', 'BC015564', 'BC015565', 'BC\n",
      "1000 input query terms found no hit:\t['BC018710', 'BC018711', 'BC018712', 'BC018714', 'BC018718', 'BC018719', 'BC018720', 'BC018722', 'BC\n",
      "1000 input query terms found no hit:\t['BC022874', 'BC022876', 'BC022877', 'BC022878', 'BC022880', 'BC022882', 'BC022890', 'BC022892', 'BC\n",
      "1000 input query terms found no hit:\t['BC028970', 'BC028973', 'BC028974', 'BC028976', 'BC028983', 'BC028985', 'BC029032', 'BC029034', 'BC\n",
      "1000 input query terms found no hit:\t['BC033016', 'BC033017', 'BC033020', 'BC033023', 'BC033024', 'BC033025', 'BC033027', 'BC033028', 'BC\n",
      "1000 input query terms found no hit:\t['BC036676', 'BC036677', 'BC036678', 'BC036679', 'BC036680', 'BC036683', 'BC036689', 'BC036701', 'BC\n",
      "1000 input query terms found no hit:\t['BC042193', 'BC042195', 'BC042196', 'BC042197', 'BC042295', 'BC042297', 'BC042333', 'BC042334', 'BC\n",
      "1000 input query terms found no hit:\t['BC050690', 'BC050695', 'BC050697', 'BC050700', 'BC050702', 'BC050704', 'BC050705', 'BC050706', 'BC\n",
      "1000 input query terms found no hit:\t['BC062571', 'BC062574', 'BC062575', 'BC062581', 'BC062582', 'BC062584', 'BC062585', 'BC062590', 'BC\n",
      "1000 input query terms found no hit:\t['BC068005', 'BC068009', 'BC068013', 'BC068025', 'BC068030', 'BC068031', 'BC068046', 'BC068050', 'BC\n",
      "1000 input query terms found no hit:\t['BC072407', 'BC072409', 'BC072410', 'BC072411', 'BC072412', 'BC072414', 'BC072416', 'BC072420', 'BC\n",
      "1000 input query terms found no hit:\t['BC093847', 'BC093848', 'BC093849', 'BC093851', 'BC093852', 'BC093853', 'BC093854', 'BC093855', 'BC\n",
      "1000 input query terms found no hit:\t['BC101592', 'BC101593', 'BC101595', 'BC101598', 'BC101599', 'BC101600', 'BC101601', 'BC101602', 'BC\n",
      "1000 input query terms found no hit:\t['BC107568', 'BC107569', 'BC107574', 'BC107575', 'BC107580', 'BC107582', 'BC107583', 'BC107584', 'BC\n",
      "1000 input query terms found no hit:\t['BC113376', 'BC113377', 'BC113378', 'BC113380', 'BC113382', 'BC113383', 'BC113385', 'BC113386', 'BC\n",
      "1000 input query terms found no hit:\t['BC122553', 'BC122555', 'BC122557', 'BC122558', 'BC122561', 'BC122564', 'BC122567', 'BC122861', 'BC\n",
      "1000 input query terms found no hit:\t['BC131783', 'BC131787', 'BC131799', 'BC131801', 'BC131805', 'BC131822', 'BC131824', 'BC131825', 'BC\n",
      "1000 input query terms found no hit:\t['BC137392', 'BC137394', 'BC137396', 'BC137397', 'BC137398', 'BC137399', 'BC137404', 'BC137405', 'BC\n",
      "15 input query terms found dup hits:\t[('BCORP1', 2), ('BCRP2', 2), ('BCRP3', 2), ('BEND3P3', 2), ('BIRC6-AS1', 2), ('BIRC8', 2), ('BMS1P1\n",
      "780 input query terms found no hit:\t['BC144609', 'BC144610', 'BC144615', 'BC144621', 'BC144627', 'BC144628', 'BC144631', 'BC144635', 'BC\n",
      "11 input query terms found dup hits:\t[('BRWD1-IT1', 2), ('BSN-AS1', 2), ('BTF3L4P1', 2), ('BTF3P10', 2), ('BTF3P11', 2), ('BTN2A3P', 2), \n",
      "747 input query terms found no hit:\t['BRWD1 antisense RNA 1', 'BRWD1 intronic transcript 1', 'BRWD1 intronic transcript 2', 'BRWD1-IT2',\n",
      "24 input query terms found dup hits:\t[('C3P1', 2), ('C4A-AS1', 6), ('C6orf47-AS1', 8), ('CA5BP1', 2), ('CACNA1C-AS3', 3), ('CACNA1C-IT1',\n",
      "377 input query terms found no hit:\t['C2orf91', 'C3 and PZP-like', 'C3H type-like 1', 'C3H1-type containing', 'C3HC-type containing 1', \n",
      "33 input query terms found dup hits:\t[('CD99P1', 3), ('CDC42-IT1', 2), ('CDR1', 2), ('CDRT15P1', 2), ('CDRT15P2', 2), ('CEACAM22P', 2), (\n",
      "165 input query terms found no hit:\t['CD6 molecule', 'CD63 molecule', 'CD68 molecule', 'CD69 molecule', 'CD7 molecule', 'CD70 molecule',\n",
      "14 input query terms found dup hits:\t[('COX11P1', 2), ('COX6A1P2', 2), ('CPHL1P', 2), ('CRIP1P4', 2), ('CROCCP2', 2), ('CRYBB2P1', 2), ('\n",
      "634 input query terms found no hit:\t['COX assembly mitochondrial protein 1 homolog', 'COX assembly mitochondrial protein 2 homolog', 'CO\n",
      "39 input query terms found dup hits:\t[('CXADRP2', 3), ('CXADRP3', 2), ('CXCR2P1', 2), ('CXXC1P1', 2), ('CYP20A1', 2), ('CYP21A1P', 5), ('\n",
      "329 input query terms found no hit:\t['CWC25 spliceosome-associated protein homolog', 'CWC27 spliceosome-associated protein homolog', 'CW\n",
      "17 input query terms found dup hits:\t[('DNM1P35', 2), ('DNM1P41', 2), ('DNM1P46', 2), ('DNM3OS', 2), ('DOC2GP', 2), ('DPH3', 2), ('DPH3P1\n",
      "894 input query terms found no hit:\t['DNL-type zinc finger', 'DNM1 pseudogene 35', 'DNM1 pseudogene 41', 'DNM1 pseudogene 46', 'DNM3 int\n",
      "17 input query terms found dup hits:\t[('DSCR4-IT1', 2), ('DSN1', 2), ('DSTNP2', 2), ('DSTNP4', 2), ('DUSP5P1', 3), ('DUX4', 2), ('DUX4L2'\n",
      "702 input query terms found no hit:\t['DQ589218', 'DQ589220', 'DQ589222', 'DQ589249', 'DQ589253', 'DQ589313', 'DQ589322', 'DQ589342', 'DQ\n",
      "7 input query terms found dup hits:\t[('EIF2S2P3', 2), ('ELK1', 2), ('ELK3', 2), ('ELK4', 2), ('EMBP1', 2), ('ENOX1-AS1', 2), ('ENOX1-AS2\n",
      "843 input query terms found no hit:\t['EIF2B5 intronic transcript 1', 'EIF2B5-AS1', 'EIF2B5-IT1', 'EIF2C1', 'EIF2C2', 'EIF2C3', 'EIF2C4',\n",
      "1000 input query terms found no hit:\t['ENST00000218867', 'ENST00000218981', 'ENST00000219022', 'ENST00000219054', 'ENST00000219066', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000239587', 'ENST00000239614', 'ENST00000239666', 'ENST00000239690', 'ENST00000239715', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000254770', 'ENST00000254799', 'ENST00000254801', 'ENST00000254803', 'ENST00000254806', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000262290', 'ENST00000262291', 'ENST00000262293', 'ENST00000262294', 'ENST00000262300', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000266556', 'ENST00000266557', 'ENST00000266560', 'ENST00000266563', 'ENST00000266564', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000278980', 'ENST00000279022', 'ENST00000279024', 'ENST00000279027', 'ENST00000279028', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000292309', 'ENST00000292314', 'ENST00000292327', 'ENST00000292330', 'ENST00000292357', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000299413', 'ENST00000299421', 'ENST00000299424', 'ENST00000299427', 'ENST00000299430', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000305885', 'ENST00000305899', 'ENST00000305901', 'ENST00000305904', 'ENST00000305910', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000311672', 'ENST00000311688', 'ENST00000311713', 'ENST00000311734', 'ENST00000311737', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000318129', 'ENST00000318130', 'ENST00000318135', 'ENST00000318149', 'ENST00000318157', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000324978', 'ENST00000324988', 'ENST00000324989', 'ENST00000325006', 'ENST00000325017', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000331173', 'ENST00000331178', 'ENST00000331194', 'ENST00000331200', 'ENST00000331203', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000336331', 'ENST00000336332', 'ENST00000336334', 'ENST00000336338', 'ENST00000336349', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000340802', 'ENST00000340806', 'ENST00000340807', 'ENST00000340811', 'ENST00000340813', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000344804', 'ENST00000344823', 'ENST00000344824', 'ENST00000344836', 'ENST00000344838', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000353147', 'ENST00000353151', 'ENST00000353159', 'ENST00000353170', 'ENST00000353172', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000356465', 'ENST00000356467', 'ENST00000356476', 'ENST00000356487', 'ENST00000356488', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000359047', 'ENST00000359051', 'ENST00000359052', 'ENST00000359059', 'ENST00000359060', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000361521', 'ENST00000361522', 'ENST00000361523', 'ENST00000361524', 'ENST00000361525', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000367174', 'ENST00000367175', 'ENST00000367176', 'ENST00000367177', 'ENST00000367178', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000368799', 'ENST00000368801', 'ENST00000368802', 'ENST00000368804', 'ENST00000368805', 'ENS\n",
      "1000 input query terms found no hit:\t['ENST00000370467', 'ENST00000370468', 'ENST00000370469', 'ENST00000370470', 'ENST00000370471', 'ENS\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='mygene.info', port=443): Read timed out. (read timeout=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:653\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 653\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:806\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    804\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 806\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:469\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:370\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='mygene.info', port=443): Read timed out. (read timeout=None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m NORMALIZE_GENE:\n\u001b[1;32m----> 2\u001b[0m     genetic_data \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_gene_symbols_in_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenetic_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 213\u001b[0m, in \u001b[0;36mnormalize_gene_symbols_in_index\u001b[1;34m(gene_df)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_gene_symbols_in_index\u001b[39m(gene_df):\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Normalize the human gene symbols at the index of a dataframe, and replace the index with its normalized version.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    Remove the rows where the index failed to be normalized.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     normalized_gene_list \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_gene_symbols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgene_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(normalized_gene_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(gene_df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    215\u001b[0m     gene_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m normalized_gene_list\n",
      "Cell \u001b[1;32mIn[1], line 201\u001b[0m, in \u001b[0;36mnormalize_gene_symbols\u001b[1;34m(gene_symbols, batch_size)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(gene_symbols), batch_size):\n\u001b[0;32m    200\u001b[0m     batch \u001b[38;5;241m=\u001b[39m gene_symbols[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 201\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquerymany\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# Update the normalized_genes dictionary with results from this batch\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m gene \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\biothings_client\\base.py:575\u001b[0m, in \u001b[0;36mBiothingClient._querymany\u001b[1;34m(self, qterms, scopes, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_fn\u001b[39m(qterms):\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_querymany_inner(qterms, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 575\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repeated_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqterms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_raw\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# hits is the raw response text\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\biothings_client\\base.py:255\u001b[0m, in \u001b[0;36mBiothingClient._repeated_query\u001b[1;34m(self, query_fn, query_li, verbose, **fn_kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquerying \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, cnt))\n\u001b[0;32m    254\u001b[0m i \u001b[38;5;241m=\u001b[39m cnt\n\u001b[1;32m--> 255\u001b[0m from_cache, query_result \u001b[38;5;241m=\u001b[39m \u001b[43mquery_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m query_result\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\biothings_client\\base.py:573\u001b[0m, in \u001b[0;36mBiothingClient._querymany.<locals>.query_fn\u001b[1;34m(qterms)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_fn\u001b[39m(qterms):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_querymany_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqterms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\biothings_client\\base.py:519\u001b[0m, in \u001b[0;36mBiothingClient._querymany_inner\u001b[1;34m(self, qterms, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    517\u001b[0m _kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m    518\u001b[0m _url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_endpoint\n\u001b[1;32m--> 519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\biothings_client\\base.py:197\u001b[0m, in \u001b[0;36mBiothingClient._post\u001b[1;34m(self, url, params, verbose)\u001b[0m\n\u001b[0;32m    195\u001b[0m return_raw \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    196\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_user_agent}\n\u001b[1;32m--> 197\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m from_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_for_status:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# raise requests.exceptions.HTTPError if not 200\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:532\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='mygene.info', port=443): Read timed out. (read timeout=None)"
     ]
    }
   ],
   "source": [
    "if NORMALIZE_GENE:\n",
    "    genetic_data = normalize_gene_symbols_in_index(genetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = geo_merge_clinical_genetic_data(selected_clinical_data, genetic_data)\n",
    "# The preprocessing runs through, which means is_available should be True\n",
    "is_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The merged dataset contains 159 samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The merged dataset contains {len(merged_data)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the feature 'Metabolic-Rate', the least common label is '0.0' with 159 occurrences. This represents 100.00% of the dataset.\n",
      "The distribution of the feature 'Metabolic-Rate' in this dataset is severely biased.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_trait_biased, merged_data = judge_and_remove_biased_features(merged_data, TRAIT, trait_type=trait_type)\n",
    "is_trait_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_available:\n",
    "    save_cohort_info(cohort, JSON_PATH, is_available, is_trait_biased, merged_data, note='')\n",
    "else:\n",
    "    save_cohort_info(cohort, JSON_PATH, is_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()\n",
    "if not is_trait_biased:\n",
    "    merged_data.to_csv(os.path.join(OUTPUT_DIR, cohort + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Do regression & Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>is_usable</th>\n",
       "      <th>is_available</th>\n",
       "      <th>is_biased</th>\n",
       "      <th>has_age</th>\n",
       "      <th>has_gender</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cohort_id, is_usable, is_available, is_biased, has_age, has_gender, sample_size, note]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the information of usable cohorts\n",
    "best_cohort, ranked_df = filter_and_rank_cohorts(JSON_PATH)\n",
    "ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If both age and gender have available cohorts, select 'age' as the condition.\n",
    "condition = 'Age'\n",
    "filter_column = 'has_' + condition.lower()\n",
    "\n",
    "condition_best_cohort, condition_ranked_df = filter_and_rank_cohorts(JSON_PATH, filter_column)\n",
    "condition_best_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_id</th>\n",
       "      <th>is_usable</th>\n",
       "      <th>is_available</th>\n",
       "      <th>is_biased</th>\n",
       "      <th>has_age</th>\n",
       "      <th>has_gender</th>\n",
       "      <th>sample_size</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cohort_id, is_usable, is_available, is_biased, has_age, has_gender, sample_size, note]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_ranked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data.drop(columns=[TRAIT, condition]).values\n",
    "Y = merged_data[TRAIT].values\n",
    "Z = merged_data[condition].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the appropriate regression model depending on whether the dataset shows batch effect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_batch_effect = detect_batch_effect(X)\n",
    "has_batch_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if has_batch_effect:\n",
    "    model_constructor1 = VariableSelection\n",
    "    model_params1 = {'modified': True, 'lamda': 3e-4}\n",
    "    model_constructor2 = VariableSelection\n",
    "    model_params2 = {'modified': False}\n",
    "else:\n",
    "    model_constructor1 = Lasso\n",
    "    model_params1 = {'alpha': 1.0, 'random_state': 42}\n",
    "    model_constructor2 = VariableSelection\n",
    "    model_params2 = {'modified': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_type = 'binary'  # Remember to set this properly, either 'binary' or 'continuous'\n",
    "cv_mean1, cv_std1 = cross_validation(X, Y, Z, model_constructor1, model_params1, target_type=trait_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mean2, cv_std2 = cross_validation(X, Y, Z, model_constructor2, model_params2, target_type=trait_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X, _ = normalize_data(X)\n",
    "normalized_Z, _ = normalize_data(Z)\n",
    "\n",
    "# Train regression model on the whole dataset to identify significant genes\n",
    "model1 = ResidualizationRegressor(model_constructor1, model_params1)\n",
    "model1.fit(normalized_X, Y, normalized_Z)\n",
    "\n",
    "model2 = ResidualizationRegressor(model_constructor2, model_params2)\n",
    "model2.fit(normalized_X, Y, normalized_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discussion and report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = merged_data.columns.tolist()\n",
    "feature_cols.remove(TRAIT)\n",
    "\n",
    "threshold = 0.05\n",
    "interpret_result(model1, feature_cols, TRAIT, condition, threshold=threshold, save_output=True,\n",
    "                 output_dir=OUTPUT_DIR, model_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_result(model2, feature_cols, TRAIT, condition, threshold=threshold, save_output=True,\n",
    "                 output_dir=OUTPUT_DIR, model_id=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
